{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b899daeb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-16T12:36:27.274052Z",
     "iopub.status.busy": "2025-03-16T12:36:27.273799Z",
     "iopub.status.idle": "2025-03-16T12:36:28.122765Z",
     "shell.execute_reply": "2025-03-16T12:36:28.122138Z"
    },
    "papermill": {
     "duration": 0.853872,
     "end_time": "2025-03-16T12:36:28.123919",
     "exception": false,
     "start_time": "2025-03-16T12:36:27.270047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/sample_submission.csv\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/AIMO_Progress_Prize_2_Reference_Problems_Solutions.pdf\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/aimo_2_gateway.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/__init__.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/aimo_2_inference_server.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/templates.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/base_gateway.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/relay.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/kaggle_evaluation.proto\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/__init__.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n",
      "/kaggle/input/ai-mathematical-olympiad-progress-prize-2/kaggle_evaluation/core/generated/__init__.py\n",
      "/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1/model.safetensors.index.json\n",
      "/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1/config.json\n",
      "/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1/model-00001-of-00002.safetensors\n",
      "/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1/model-00002-of-00002.safetensors\n",
      "/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1/tokenizer.json\n",
      "/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1/tokenizer_config.json\n",
      "/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1/special_tokens_map.json\n",
      "/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f2e615",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T12:36:28.130109Z",
     "iopub.status.busy": "2025-03-16T12:36:28.129749Z",
     "iopub.status.idle": "2025-03-16T12:37:30.146093Z",
     "shell.execute_reply": "2025-03-16T12:37:30.145321Z"
    },
    "papermill": {
     "duration": 62.020511,
     "end_time": "2025-03-16T12:37:30.147303",
     "exception": false,
     "start_time": "2025-03-16T12:36:28.126792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-16 12:37:24 __init__.py:183] Automatically detected platform cuda.\n",
      "PyTorch version: 2.5.1+cu124\n",
      "vLLM: 0.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import numpy as np, pandas as pd, polars as pl\n",
    "\n",
    "import torch\n",
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('vLLM:', vllm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e52ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T12:37:30.153365Z",
     "iopub.status.busy": "2025-03-16T12:37:30.153124Z",
     "iopub.status.idle": "2025-03-16T12:37:30.166568Z",
     "shell.execute_reply": "2025-03-16T12:37:30.165937Z"
    },
    "papermill": {
     "duration": 0.017603,
     "end_time": "2025-03-16T12:37:30.167654",
     "exception": false,
     "start_time": "2025-03-16T12:37:30.150051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=0)\n",
    "\n",
    "start_time = time.time()\n",
    "cutoff_time = start_time + (4 * 60 + 45) * 60\n",
    "cutoff_times = [int(x) for x in np.linspace(cutoff_time, start_time + 60 * 60, 50 + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435818d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T12:37:30.173345Z",
     "iopub.status.busy": "2025-03-16T12:37:30.173127Z",
     "iopub.status.idle": "2025-03-16T12:39:17.896121Z",
     "shell.execute_reply": "2025-03-16T12:39:17.895301Z"
    },
    "papermill": {
     "duration": 107.727499,
     "end_time": "2025-03-16T12:39:17.897661",
     "exception": false,
     "start_time": "2025-03-16T12:37:30.170162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-16 12:37:58 config.py:526] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 03-16 12:38:01 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 03-16 12:38:01 config.py:1383] Defaulting to use mp for distributed inference\n",
      "WARNING 03-16 12:38:01 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-16 12:38:01 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1', speculative_config=None, tokenizer='/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=12288, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=2024, served_model_name=/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[32,24,16,8,4,2,1],\"max_capture_size\":32}, use_cached_outputs=False, \n",
      "WARNING 03-16 12:38:02 multiproc_worker_utils.py:298] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-16 12:38:02 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-16 12:38:02 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m INFO 03-16 12:38:02 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-16 12:38:02 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-16 12:38:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:38:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:38:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m WARNING 03-16 12:38:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-16 12:38:03 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-16 12:38:03 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-16 12:38:03 cuda.py:235] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-16 12:38:03 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 03-16 12:38:14 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 03-16 12:38:14 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m INFO 03-16 12:38:14 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 03-16 12:38:14 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m INFO 03-16 12:38:14 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "INFO 03-16 12:38:14 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-16 12:38:14 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-16 12:38:14 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "WARNING 03-16 12:38:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m WARNING 03-16 12:38:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-16 12:38:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 03-16 12:38:15 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 03-16 12:38:15 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ae792ec6'), local_subscribe_port=43067, remote_subscribe_port=None)\n",
      "INFO 03-16 12:38:15 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m INFO 03-16 12:38:15 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n",
      "INFO 03-16 12:38:15 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n",
      "INFO 03-16 12:38:15 model_runner.py:1111] Starting to load model /kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69b28f5b8be482f9a940b5b70a030aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-16 12:38:45 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m INFO 03-16 12:38:45 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "INFO 03-16 12:38:45 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-16 12:38:45 model_runner.py:1116] Loading model weights took 1.3375 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-16 12:39:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:39:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m WARNING 03-16 12:39:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:39:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:39:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-16 12:39:02 worker.py:266] Memory profiling takes 16.73 seconds\r\n",
      "WARNING 03-16 12:39:02 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-16 12:39:02 worker.py:266] Memory profiling takes 16.73 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m INFO 03-16 12:39:02 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 03-16 12:39:02 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 03-16 12:39:02 worker.py:266] Memory profiling takes 16.73 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-16 12:39:02 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.78GiB; the rest of the memory reserved for KV Cache is 18.88GiB.\n",
      "INFO 03-16 12:39:02 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.78GiB; the rest of the memory reserved for KV Cache is 18.88GiB.\n",
      "INFO 03-16 12:39:02 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m INFO 03-16 12:39:02 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.78GiB; the rest of the memory reserved for KV Cache is 18.88GiB.\n",
      "WARNING 03-16 12:39:03 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:39:03 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-16 12:39:03 worker.py:266] Memory profiling takes 17.08 seconds\r\n",
      "INFO 03-16 12:39:03 worker.py:266] the current vLLM instance can use total_gpu_memory (22.28GiB) x gpu_memory_utilization (0.95) = 21.16GiB\r\n",
      "INFO 03-16 12:39:03 worker.py:266] model weights take 1.34GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 0.78GiB; the rest of the memory reserved for KV Cache is 18.88GiB.\n",
      "INFO 03-16 12:39:03 executor_base.py:108] # CUDA blocks: 88385, # CPU blocks: 18724\n",
      "INFO 03-16 12:39:03 executor_base.py:113] Maximum concurrency for 12288 tokens per request: 115.08x\n",
      "WARNING 03-16 12:39:03 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:39:03 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-16 12:39:03 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:39:03 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:39:03 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m WARNING 03-16 12:39:03 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:39:03 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "WARNING 03-16 12:39:03 config.py:975] MLA is not supported with awq_marlin quantization. Disabling MLA.\n",
      "INFO 03-16 12:39:08 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-16 12:39:08 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-16 12:39:08 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-16 12:39:08 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  86%|████████▌ | 6/7 [00:05<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=351)\u001b[0;0m INFO 03-16 12:39:15 model_runner.py:1563] Graph capturing finished in 7 secs, took 0.10 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=352)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 7/7 [00:07<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-16 12:39:15 model_runner.py:1563] Graph capturing finished in 7 secs, took 0.10 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 7/7 [00:07<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-16 12:39:15 model_runner.py:1563] Graph capturing finished in 7 secs, took 0.10 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=359)\u001b[0;0m INFO 03-16 12:39:15 model_runner.py:1563] Graph capturing finished in 7 secs, took 0.10 GiB\n",
      "INFO 03-16 12:39:15 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 29.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.getenv('KAGGLE_KERNEL_RUN_TYPE') or os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    llm_model_pth = '/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b-awq-casperhansen/1'\n",
    "else:\n",
    "    llm_model_pth = '/root/volume/KirillR/QwQ-32B-Preview-AWQ'\n",
    "\n",
    "MAX_NUM_SEQS = 32\n",
    "MAX_MODEL_LEN = 8192 * 3 // 2\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "#    dtype=\"half\",                 # The data type for the model weights and activations\n",
    "    max_num_seqs=MAX_NUM_SEQS,    # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=MAX_MODEL_LEN,  # Model context length\n",
    "    trust_remote_code=True,       # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=4,       # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.95,  # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    "    seed=2024,\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbc04b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T12:39:17.917376Z",
     "iopub.status.busy": "2025-03-16T12:39:17.917102Z",
     "iopub.status.idle": "2025-03-16T12:39:17.926582Z",
     "shell.execute_reply": "2025-03-16T12:39:17.925970Z"
    },
    "papermill": {
     "duration": 0.02038,
     "end_time": "2025-03-16T12:39:17.927603",
     "exception": false,
     "start_time": "2025-03-16T12:39:17.907223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\"\n",
    "\n",
    "def batch_message_filter(list_of_messages) -> tuple[list[list[dict]], list[str]]:\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    for messages in list_of_messages:\n",
    "        answer = extract_boxed_text(messages[-1]['content'])\n",
    "        if answer:\n",
    "            extracted_answers.append(answer)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "    return list_of_messages_to_keep, extracted_answers\n",
    "\n",
    "def select_answer(answers):\n",
    "    counter = Counter()\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                counter[int(answer)] += 1 + random.random() / 1_000\n",
    "        except:\n",
    "            pass\n",
    "    if not counter:\n",
    "        return 210\n",
    "    _, answer = sorted([(v,k) for k,v in counter.items()], reverse=True)[0]\n",
    "    return answer%1000\n",
    "\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "    max_tokens = MAX_MODEL_LEN\n",
    "    if time.time() > cutoff_times[-1]:\n",
    "        print(\"Speedrun\")\n",
    "        max_tokens = 2 * MAX_MODEL_LEN // 3\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=1.0,               # Randomness of the sampling\n",
    "        top_p=0.90,                    # Cumulative probability of the top tokens to consider\n",
    "        min_p=0.05,                    # Minimum probability for a token to be considered\n",
    "        skip_special_tokens=True,      # Whether to skip special tokens in the output\n",
    "        max_tokens=max_tokens,         # Maximum number of tokens to generate\n",
    "        stop=[\"</think>\"],             # List of strings that stop the generation\n",
    "        seed=777,\n",
    "    )\n",
    "    \n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "\n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    print([len(single_request_output.outputs[0].token_ids) for single_request_output in request_output])\n",
    "\n",
    "    sort_keys_and_list_of_messages = []\n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        #print()\n",
    "        #print(single_request_output.outputs[0].text)\n",
    "        #print()\n",
    "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
    "\n",
    "        sort_keys_and_list_of_messages.append(\n",
    "            (\n",
    "                len(single_request_output.outputs[0].token_ids),\n",
    "                messages\n",
    "            )\n",
    "        )\n",
    "    print([sort_key for sort_key, _ in sort_keys_and_list_of_messages])\n",
    "    sort_keys_and_list_of_messages.sort(key=lambda sort_key_and_messages: sort_key_and_messages[0])\n",
    "    print([sort_key for sort_key, _ in sort_keys_and_list_of_messages])\n",
    "    \n",
    "    list_of_messages = [messages for _, messages in sort_keys_and_list_of_messages]\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d693ac9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T12:39:17.946415Z",
     "iopub.status.busy": "2025-03-16T12:39:17.946196Z",
     "iopub.status.idle": "2025-03-16T12:39:17.953901Z",
     "shell.execute_reply": "2025-03-16T12:39:17.953327Z"
    },
    "papermill": {
     "duration": 0.018214,
     "end_time": "2025-03-16T12:39:17.954909",
     "exception": false,
     "start_time": "2025-03-16T12:39:17.936695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_starter_messages(question, index):\n",
    "    options = []\n",
    "    for _ in range(13):\n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step. Return final answer within \\\\boxed{}, after taking modulo 1000.\"},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ]\n",
    "        )\n",
    "    for _ in range(3):    \n",
    "        options.append(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step. After you get your final answer, take modulo 1000, and return the final answer within \\\\boxed{}.\"},\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "    return options[index%len(options)]\n",
    "\n",
    "def predict_for_question(question: str) -> int:\n",
    "    selected_questions_only = True\n",
    "    #selected_questions_only = False\n",
    "    if selected_questions_only and not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        #if \"Triangle\" not in question:\n",
    "        #    return 210\n",
    "        if \"Triangle\" not in question and \"delightful\" not in question and \"George\" not in question:\n",
    "            return 210\n",
    "\n",
    "    if time.time() > cutoff_time:\n",
    "        return 210\n",
    "    \n",
    "    print(question)\n",
    "\n",
    "    num_seqs = MAX_NUM_SEQS\n",
    "    if time.time() > cutoff_times[-1]:\n",
    "        num_seqs = 2 * MAX_NUM_SEQS // 3\n",
    "    \n",
    "    list_of_messages = [create_starter_messages(question, index) for index in range(num_seqs)]\n",
    "\n",
    "    all_extracted_answers = []\n",
    "    for _ in range(1):\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        \n",
    "        if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                    \"question\": [question] * len(list_of_messages),\n",
    "                    \"message\": [messages[-1][\"content\"] for messages in list_of_messages],\n",
    "                }\n",
    "            )\n",
    "            df.to_csv(f\"{str(int(time.time() - start_time)).zfill(5)}.csv\", index=False)\n",
    "        \n",
    "        list_of_messages, extracted_answers = batch_message_filter(list_of_messages)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "    \n",
    "    print(all_extracted_answers)\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    cutoff_times.pop()\n",
    "    return answer\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c1e5dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T12:39:17.973443Z",
     "iopub.status.busy": "2025-03-16T12:39:17.973227Z",
     "iopub.status.idle": "2025-03-16T12:39:18.004147Z",
     "shell.execute_reply": "2025-03-16T12:39:18.003514Z"
    },
    "papermill": {
     "duration": 0.04148,
     "end_time": "2025-03-16T12:39:18.005374",
     "exception": false,
     "start_time": "2025-03-16T12:39:17.963894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee34ed77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T12:39:18.023828Z",
     "iopub.status.busy": "2025-03-16T12:39:18.023611Z",
     "iopub.status.idle": "2025-03-16T12:59:11.532052Z",
     "shell.execute_reply": "2025-03-16T12:59:11.531290Z"
    },
    "papermill": {
     "duration": 1193.518866,
     "end_time": "2025-03-16T12:59:11.533199",
     "exception": false,
     "start_time": "2025-03-16T12:39:18.014333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "1acac0\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [05:26<00:00, 10.19s/it, est. speed input: 10.06 toks/s, output: 674.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6039, 6653, 8467, 5939, 6545, 5848, 7684, 6513, 6292, 12188, 8449, 6203, 6927, 6721, 6952, 7482, 4481, 5502, 5502, 8004, 5878, 5254, 12188, 4967, 5740, 4850, 3828, 7529, 5392, 11455, 4326, 10344]\n",
      "[6039, 6653, 8467, 5939, 6545, 5848, 7684, 6513, 6292, 12188, 8449, 6203, 6927, 6721, 6952, 7482, 4481, 5502, 5502, 8004, 5878, 5254, 12188, 4967, 5740, 4850, 3828, 7529, 5392, 11455, 4326, 10344]\n",
      "[3828, 4326, 4481, 4850, 4967, 5254, 5392, 5502, 5502, 5740, 5848, 5878, 5939, 6039, 6203, 6292, 6513, 6545, 6653, 6721, 6927, 6952, 7482, 7529, 7684, 8004, 8449, 8467, 10344, 11455, 12188, 12188]\n",
      "['180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180', '180']\n",
      "180\n",
      "\n",
      "\n",
      "\n",
      "Triangle $ABC$ has side length $AB = 120$ and circumradius $R = 100$. Let $D$ be the foot of the perpendicular from $C$ to the line $AB$. What is the greatest possible length of segment $CD$?\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "88c219\n",
      "For positive integers $x_1,\\ldots, x_n$ define $G(x_1, \\ldots, x_n)$ to be the sum of their $\\frac{n(n-1)}{2}$ pairwise greatest common divisors. We say that an integer $n \\geq 2$ is \\emph{artificial} if there exist $n$ different positive integers $a_1, ..., a_n$ such that \n",
      "\\[a_1 + \\cdots + a_n = G(a_1, \\ldots, a_n) +1.\\]\n",
      "Find the sum of all artificial integers $m$ in the range $2 \\leq m \\leq 40$.\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "349493\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [07:26<00:00, 13.96s/it, est. speed input: 10.78 toks/s, output: 862.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12070, 12140, 12140, 12140, 11057, 12140, 12140, 12140, 12140, 12140, 11269, 12140, 12140, 12132, 12132, 12132, 11791, 12140, 12140, 12140, 12140, 12140, 11373, 12140, 12140, 12140, 12140, 12140, 12140, 12132, 12132, 12132]\n",
      "[12070, 12140, 12140, 12140, 11057, 12140, 12140, 12140, 12140, 12140, 11269, 12140, 12140, 12132, 12132, 12132, 11791, 12140, 12140, 12140, 12140, 12140, 11373, 12140, 12140, 12140, 12140, 12140, 12140, 12132, 12132, 12132]\n",
      "[11057, 11269, 11373, 11791, 12070, 12132, 12132, 12132, 12132, 12132, 12132, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140, 12140]\n",
      "['0', '2', '0', '0', '2']\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "We call a sequence $a_1, a_2, \\ldots$ of non-negative integers \\textit{delightful} if there exists a positive integer $N$ such that for all $n > N$, $a_n = 0$, and for all $i \\geq 1$, $a_i$ counts the number of multiples of $i$ in $a_1, a_2, \\ldots, a_N$. How many delightful sequences of non-negative integers are there?\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "192e23\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 32/32 [06:59<00:00, 13.12s/it, est. speed input: 10.02 toks/s, output: 874.06 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12159, 11959, 12159, 9458, 12159, 9956, 12159, 12159, 10420, 12159, 9610, 12159, 12159, 10802, 12151, 12151, 6768, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 6871, 12159, 12159, 12159, 12159, 12151, 12151, 11623]\n",
      "[12159, 11959, 12159, 9458, 12159, 9956, 12159, 12159, 10420, 12159, 9610, 12159, 12159, 10802, 12151, 12151, 6768, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 6871, 12159, 12159, 12159, 12159, 12151, 12151, 11623]\n",
      "[6768, 6871, 9458, 9610, 9956, 10420, 10802, 11623, 11959, 12151, 12151, 12151, 12151, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159, 12159]\n",
      "['250', '250', '750', '0', '250', '750', '750', '0', '750']\n",
      "750\n",
      "\n",
      "\n",
      "\n",
      "Fred and George take part in a tennis tournament with $4046$ other players. In each round, the players are paired into $2024$ matches. How many ways are there to arrange the first round such that Fred and George do not have to play each other? (Two arrangements for the first round are \\textit{different} if there is a player with a different opponent in the two arrangements.)\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "a1d40b\n",
      "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$. There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not. How many prime factors does $N$ have, counted with multiplicity?\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "057f8a\n",
      "Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "bbd91e\n",
      "Alice writes all positive integers from $1$ to $n$ on the board for some positive integer $n \\geq 11$. Bob then erases ten of them. The mean of the remaining numbers is $3000/37$. The sum of the numbers Bob erased is $S$. What is the remainder when $n \\times S$ is divided by $997$?\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "480182\n",
      "Let $ABC$ be a triangle with $BC=108$, $CA=126$, and $AB=39$. Point $X$ lies on segment $AC$ such that $BX$ bisects $\\angle CBA$. Let $\\omega$ be the circumcircle of triangle $ABX$. Let $Y$ be a point on $\\omega$ different from $X$ such that $CX=CY$. Line $XY$ meets $BC$ at $E$. The length of the segment $BE$ can be written as $\\frac{m}{n}$, where $m$ and $n$ are coprime positive integers. Find $m+n$.\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "1fce4b\n",
      "Find the three-digit number $n$ such that writing any other three-digit number $10^{2024}$ times in a row and $10^{2024}+2$ times in a row results in two numbers divisible by $n$.\n",
      "------\n",
      "\n",
      "\n",
      "------\n",
      "71beb6\n",
      "For a positive integer $n$, let $S(n)$ denote the sum of the digits of $n$ in base 10. Compute $S(S(1)+S(2)+\\cdots+S(N))$ with $N=10^{100}-2$.\n",
      "------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "#            '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv',\n",
    "            'reference.csv',\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11376393,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "sourceId": 220483900,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 224053,
     "modelInstanceId": 206829,
     "sourceId": 242129,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1372.939088,
   "end_time": "2025-03-16T12:59:16.367145",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-16T12:36:23.428057",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "13ebbe39a1204e1e9fdf7ac4accd3190": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2160f2964e024e38935e0b64a80851de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2a6f8ddeb2d541d5b035c912b9161778": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4cdd9be88b594e23b50ceb28e30007db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "53da79d5f52d4035947bb2744819958d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7fd55ae05d51461fa3767027926364f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2a6f8ddeb2d541d5b035c912b9161778",
       "placeholder": "​",
       "style": "IPY_MODEL_53da79d5f52d4035947bb2744819958d",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "88317f1ab2df43208c99b882b84903db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_13ebbe39a1204e1e9fdf7ac4accd3190",
       "placeholder": "​",
       "style": "IPY_MODEL_2160f2964e024e38935e0b64a80851de",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:29&lt;00:00, 12.62s/it]\n"
      }
     },
     "89d9f134e40d47b0bc3e249464927835": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9518aba85ce241b3862175d65e096540": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "abb035dc72bf4060b885e318f0b2bc70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_89d9f134e40d47b0bc3e249464927835",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4cdd9be88b594e23b50ceb28e30007db",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "d69b28f5b8be482f9a940b5b70a030aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7fd55ae05d51461fa3767027926364f3",
        "IPY_MODEL_abb035dc72bf4060b885e318f0b2bc70",
        "IPY_MODEL_88317f1ab2df43208c99b882b84903db"
       ],
       "layout": "IPY_MODEL_9518aba85ce241b3862175d65e096540",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
